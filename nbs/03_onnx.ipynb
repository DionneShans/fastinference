{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp onnx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ONNX\n",
    "> Exporting models to `ONNX` format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#slow\n",
    "# export\n",
    "from fastai2.learner import Learner\n",
    "import onnxruntime as ort\n",
    "from fastcore.all import *\n",
    "import torch\n",
    "from torch import tensor, Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#slow\n",
    "#export\n",
    "@patch\n",
    "def to_onnx(x:Learner, fname='export', path=Path('.')):\n",
    "    \"Export model to `ONNX` format\"\n",
    "    orig_bs = x.dls[0].bs\n",
    "    x.dls[0].bs=1\n",
    "    dummy_inp = next(iter(x.dls[0]))\n",
    "    x.dls[0].bs = orig_bs\n",
    "    names = inspect.getfullargspec(x.model.forward).args[1:]\n",
    "    dynamic_axes = {n:{0:'batch_size'} for n in names}\n",
    "    dynamic_axes['output'] = {0:'batch_size'}\n",
    "    torch.onnx.export(x.model, dummy_inp[:-1], path/f'{fname}.onnx',\n",
    "                     input_names=names, output_names=['output'],\n",
    "                     dynamic_axes=dynamic_axes)\n",
    "    data_exp = x.dls.new_empty()\n",
    "    data_exp.loss_func = x.loss_func\n",
    "    torch.save(data_exp, path/f'{fname}.pkl', pickle_protocol=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h4 id=\"Learner.to_onnx\" class=\"doc_header\"><code>Learner.to_onnx</code><a href=\"__main__.py#L3\" class=\"source_link\" style=\"float:right\">[source]</a></h4>\n",
       "\n",
       "> <code>Learner.to_onnx</code>(**`x`**:`Learner`, **`fname`**=*`'export'`*, **`path`**=*`Path('.')`*)\n",
       "\n",
       "Export model to `ONNX` format"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#slow\n",
    "show_doc(Learner.to_onnx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Currently supports single-output models. See an example usage below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai2.tabular.all import *\n",
    "path = untar_data(URLs.ADULT_SAMPLE)\n",
    "df = pd.read_csv(path/'adult.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = RandomSplitter()(range_of(df))\n",
    "cat_names = ['workclass', 'education', 'marital-status', 'occupation', 'relationship', 'race']\n",
    "cont_names = ['age', 'fnlwgt', 'education-num']\n",
    "procs = [Categorify, FillMissing, Normalize]\n",
    "y_names = 'salary'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dls = TabularPandas(df, procs=procs, cat_names=cat_names, cont_names=cont_names,\n",
    "                   y_names=y_names, splits=splits).dataloaders()\n",
    "learn = tabular_learner(dls, layers=[200,100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#slow\n",
    "learn.to_onnx('tabular')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#slow\n",
    "#export\n",
    "from fastinference.inference.inference import _decode_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#slow\n",
    "#export\n",
    "class fastONNX():\n",
    "    \"ONNX wrapper for `Learner`\"\n",
    "    def __init__(self, fn):\n",
    "        self.ort_session = ort.InferenceSession(fn+'.onnx')\n",
    "        try:\n",
    "            self.ort_session.set_providers(['CUDAExecutionProvider'])\n",
    "            cpu = False\n",
    "        except:\n",
    "            self.ort_session.set_providers(['CPUExecutionProvider'])\n",
    "            cpu = True\n",
    "        self.dls = torch.load(fn+'.pkl')\n",
    "    \n",
    "    def to_numpy(self, t:tensor): return t.detach.cpu().numpy() if t.requires_grad else t.cpu().numpy()\n",
    "    \n",
    "    def predict(self, inps):\n",
    "        \"Predict a single numpy item\"\n",
    "        if isinstance(inps[0], Tensor): inps = [self.to_numpy(x) for x in inps]\n",
    "        names = [i.name for i in self.ort_session.get_inputs()]\n",
    "        xs = {name:x for name,x in zip(names,inps)}\n",
    "        outs = self.ort_session.run(None, xs)\n",
    "        return outs\n",
    "    \n",
    "    def get_preds(self, dl=None, raw_outs=False, decoded_loss=True, fully_decoded=False):\n",
    "        \"Get predictions with possible decoding\"\n",
    "        inps, outs, dec_out, raw = [], [], [], []\n",
    "        loss_func = self.dls.loss_func\n",
    "        is_multi, n_inp = False, self.dls.n_inp\n",
    "        if n_inp > 1:\n",
    "            is_multi = true\n",
    "            [inps.append([]) for _ in range(n_inp)]\n",
    "        for batch in dl:\n",
    "            batch_np = []\n",
    "            if is_multi:\n",
    "                for i in range(n_inp):\n",
    "                    item = self.to_numpy(batch[i])\n",
    "                    inps[i].append(item)\n",
    "                    batch_np.append(item)\n",
    "            else:\n",
    "                inps.append(self.to_numpy(batch[:n_inp]))\n",
    "            if decoded_loss or fully_decoded:\n",
    "                out = self.predict(batch_np)\n",
    "                raw.append(out)\n",
    "                dec_out.append(loss_func.decodes(tensor(out)))\n",
    "            else:\n",
    "                raw.append(self.predict(batch_np))\n",
    "        axis = 1 if len(dl) > 1 else 0\n",
    "        raw = np.concatenate(raw, axis=axis)\n",
    "        if decoded_loss or fully_decoded:\n",
    "            dec_out = np.concatenate(dec_out, axis=axis)\n",
    "        if not raw_outs:\n",
    "            try: outs.insert(0, loss_func.activation(tensor(raw)).numpy())\n",
    "            except: outs.insert(0, dec_out)\n",
    "        else:\n",
    "            outs.insert(0, raw)\n",
    "        if decoded_loss: outs = _decode_loss(self.dls.vocab, dec_out, outs)\n",
    "        return outs\n",
    "    \n",
    "    def test_dl(self, test_items, **kwargs): return self.dls.test_dl(test_items, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<h2 id=\"fastONNX\" class=\"doc_header\"><code>class</code> <code>fastONNX</code><a href=\"\" class=\"source_link\" style=\"float:right\">[source]</a></h2>\n",
       "\n",
       "> <code>fastONNX</code>(**`fn`**)\n",
       "\n",
       "ONNX wrapper for `Learner`"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#slow\n",
    "show_doc(fastONNX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
