# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/00b_inference.export.ipynb (unless otherwise specified).

__all__ = ['get_information']

# Cell
from fastai2.vision.all import *

# Cell
def _gen_dict(tfm):
    "Grabs the `attrdict` and transform name from `tfm`"
    tfm_dict = attrdict(tfm, *tfm.store_attrs.split(','))
    if 'partial' in tfm.name:
        tfm_name = tfm.name[1].split(' --')[0]
    else:
        tfm_name = tfm.name.split(' --')[0]
    return tfm_dict, tfm_name

# Cell
def _make_tfm_dict(tfms, type_tfm=False):
    "Extracts transform params from `tfms`"
    tfm_dicts = {}
    for tfm in tfms:
        if hasattr(tfm, 'store_attrs') and not isinstance(tfm, AffineCoordTfm):
            if type_tfm or tfm.split_idx is not 0:
                tfm_dict,name = _gen_dict(tfm)
                tfm_dicts[name] = tfm_dict
    return tfm_dicts

# Cell
@typedispatch
def _extract_tfm_dicts(dl:TfmdDL):
    "Extracts all transform params from `dl`"
    type_tfm = True
    attrs = ['tfms','after_item','after_batch']
    tfm_dicts = {}
    for attr in attrs:
        tfm_dicts[attr] = _make_tfm_dict(getattr(dl, attr), type_tfm)
        type_tfm = False
    return tfm_dicts

# Cell
def get_information(dls): return _extract_tfm_dicts(dls[0])

# Cell
from fastai2.tabular.all import *

# Cell
@typedispatch
def _extract_tfm_dicts(dl:TabDataLoader):
    "Extracts all transform params from `dl`"
    types = 'normalize,fill_missing,categorify'
    if hasattr(dl, 'categorize'): types += ',categorize'
    if hasattr(dl, 'regression_setup'): types += ',regression_setup'
    tfms = {}
    for t in types.split(','):
        tfm = getattr(dl, t)
        tfms[t] = attrdict(tfm, *tfm.store_attrs.split(','))
    return tfms

# Cell
@patch
def to_fastinference(x:Learner, data_fname='data', model_fname='model', path=Path('.')):
    "Export data for `fastinference_onnx` or `_pytorch` to use"
    dicts = get_information(x.dls)
    with open(path/f'{data_fname}.pkl', 'wb') as handle:
        pickle.dump(procs, handle, protocol=pickle.HIGHEST_PROTOCOL)
    x._end_cleanup()
    state = x.opt.state_dict() if x.opt is not None else None
    x.opt = None
    torch.save(x.model.state_dict(), path/f'{model_fname}.pkl')